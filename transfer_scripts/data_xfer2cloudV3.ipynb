{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Data Uploader for UFS Datasets to Cloud Data Storage\n",
    "\n",
    "### __Purpose:__ \n",
    "\n",
    "The purpose of this program is to transfer the input and baseline datasets residing within the RDHPCS to cloud data storage via chaining API calls to communicate with cloud data storage buckets. The program will support the data required for the current UFS-WM deployed within the CSPs as well as support the NOAA development team's data management in maintaining only the datasets committed within the latest N months of their UFS development code (once the program is integrated into Jenkins).\n",
    "\n",
    "According to Amazon AWS, the following conditions need to be considered when transferring data to cloud data storage:\n",
    "- Largest object that can be uploaded in a single PUT is 5 GB.\n",
    "- Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB.\n",
    "- For objects larger than 100 MB, Amazon recommends using the Multipart Upload capability.\n",
    "- The total volume of data in a cloud data storage bucket are unlimited.\n",
    "\n",
    "Tools which could be be utilized to perform data transferring & partitioning (Multipart Upload/Download) are: \n",
    "- AWS SDK\n",
    "- AWS CLI\n",
    "- AWS S3 REST API\n",
    "\n",
    "All of the AWS provided tools are built on Boto3. \n",
    "\n",
    "In this demontration, the framework will implement Python AWS SDK for transferring the UFS datasets from the RDHPCS, Orion, to the cloud data storage with low latency. \n",
    "\n",
    "The AWS SDK will be implemented for the following reasons:\n",
    "- To integrate with other python scripts.\n",
    "- AWS SDK carries addition capabilities/features for data manipulation & transferring compare to the aforementioned alternate tools.\n",
    "\n",
    "### __Capabilities:__ \n",
    "\n",
    "The framework will be able to perform the following actions:\n",
    "\n",
    "- Apply multi-threading & partitioning to the datasets to assist in the optimization in uploading performance of the datasets from on-prem to cloud. \n",
    "\n",
    "### __Future Capabilities:__  \n",
    "\n",
    "The program can be used as a skeletal framework for transferring future datasets of interest (e.g. SRW data, MRW data, etc). In addition, it can be integrated with the UFS tracker bot (https://github.com/NOAA-EPIC/ufs-dev_data_timestamps) & Jenkins to automate the data transferring process as new datasets are being committed & pushed to the UFS-WM repository develop branch.\n",
    "\n",
    "\n",
    "### __Sample Datasets to Transfer:__\n",
    "There are two scenarios that will need to be considered when storing data in cloud:\n",
    "\n",
    "- Datasets to be stored need to support NOAA's development team. Datasets residing within the Cloud as well as RDHPCS must support their development team's latest 2 months of developing code. \n",
    "\n",
    "\n",
    "| UFS MODEL DEVELOPMENT VERSIONS| BASELINE DATA | INPUT DATA | WW3 INPUT DATA | BM_IC |\n",
    "| :- | :- | :- | -: | :-: |\n",
    "| Supports NOAA Dev Team Versons (since 03-16-22)| 20220316 | input-data-20211210  | WW3_input_data_20211113 | BM_IC-20220207 |\n",
    "| Supports NOAA Dev Team Versons (since 03-18-22)| 20220318 | input-data-20211210  | WW3_input_data_20211113 | BM_IC-20220207 |\n",
    "| Supports NOAA Dev Team Versions (since 03-18-22)| 20220321 | input-data-20211210  | WW3_input_data_20211113 | BM_IC-20220207 |\n",
    "\n",
    "- Datasets to be stored need to support the UFS weather model develop branch code revision, which was pulled last year October 2021 by the EPIC's Platform team. These datasets are:\n",
    "\n",
    "| UFS MODEL DEVELOPMENT VERSIONS| BASELINE DATA | INPUT DATA | WW3 INPUT DATA | BM_IC |\n",
    "| :- | :- | :- | -: | :-: |\n",
    "| Supports UFS Model Version Deployed in CSPs| 20220207 | input-data-20211210  | WW3_input_data_20211113 | BM_IC-20210717 |\n",
    "\n",
    "<img src=\"./images/DataVersionsZach&JongAreUsing.png\">\n",
    "\n",
    "### __Environment Setup:__\n",
    "\n",
    "1. Install miniconda on your machine. Note: Miniconda is a smaller version of Anaconda that only includes conda along with a small set of necessary and useful packages. With Miniconda, you can install only what you need, without all the extra packages that Anaconda comes packaged with:\n",
    "\n",
    "Download latest Miniconda (e.g. 3.9 version):\n",
    "- __wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Check integrity downloaded file with SHA-256:\n",
    "- __sha256sum Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Reference SHA256 hash in following link: https://docs.conda.io/en/latest/miniconda.html\n",
    "\n",
    "Install Miniconda in Linux:\n",
    "- __bash Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Next, Miniconda installer will prompt where do you want to install Miniconda. Press ENTER to accept the default install location i.e. your $HOME directory. If you don't want to install in the default location, press CTRL+C to cancel the installation or mention an alternate installation directory. If you've chosen the default location, the installer will display “PREFIX=/var/home/<user>/miniconda3” and continue the installation.\n",
    "\n",
    "For installation to take into effect, run the following command: \n",
    "- __source ~/.bashrc__\n",
    "\n",
    "Next, you will see the prefix (base) in front of your terminal/shell prompt. Indicating the conda's base environment is activated.\n",
    "\n",
    "2.\tOnce you have conda installed on your machine, perform the following to create a conda environment:\n",
    "\n",
    "To create a new environment (if a YAML file is not provided)\n",
    "- __conda create -n [Name of your conda environment you wish to create]__\n",
    "\n",
    "__(OR)__\n",
    "\n",
    "To ensure you are running Python 3.9:\n",
    "- __conda create -n myenv Python=3.9__\n",
    "\n",
    "__(OR)__\n",
    "\n",
    "To create a new environment from an existing YAML file (if a YAML file is provided):\n",
    "- __conda env create -f environment.yml__\n",
    "\n",
    "__*Note:__ A .yml file is a text file that contains a list of dependencies, which channels a list for installing dependencies for the given conda environment. For the code to utilize the dependencies, you will need to be in the directory where the environment.yml file lives.\n",
    "\n",
    "4.\tActivate the new environment via: \n",
    "- __conda activate [Name of your conda environment you wish to activate]__\n",
    "\n",
    "5.\tVerify that the new environment was installed correctly via:\n",
    "- __conda info --env__\n",
    "\n",
    "__*Note:__\n",
    "- From this point on, must activate conda environment prior to .py script(s) or jupyter notebooks execution\n",
    "using the following command: __conda activate__\n",
    "- To deactivate a conda environment: \n",
    "    - __conda deactivate__\n",
    "\n",
    "#### ___Link Home Directory to Dataset Location on RDHPCS Platform___ \n",
    "\n",
    "6.\tUnfortunately, there is no way to navigate to the /work/ filesystem from within the Jupyter interface. The best way to workaround is to create a symbolic link in your home folder that will take you to the /work/ filesystem. Run the following command from a linux terminal on Orion to create the link: \n",
    "\n",
    "    - __ln -s /work /home/[Your user account name]/work__\n",
    "\n",
    "Now, when you navigate to the __/home/[Your user account name]/work__ directory in Jupyter, it will take you to the __/work__ folder. Allowing you to obtain any data residing within the __/work__ filesystem that you have permission to access from Jupyter. This same procedure will work for any filesystem available from the root directory. \n",
    "\n",
    "__*Note:__ On Orion, user must sym link from their home directory to the main directory containing the datasets of interest.\n",
    "\n",
    "#### ___Open & Run Data Analytics Tool on Jupyter Notebook___\n",
    "\n",
    "7.\tOpen OnDemand has a built-in file explorer and file transfer application available directly from its dashboard via ...\n",
    "    - Login to https://orion-ood.hpc.msstate.edu/ \n",
    "    - In the Open OnDemand Interface, select __Interactive Apps__ > __Jupyter Notbook__\n",
    "    - Set the following configurations to run Jupyter:\n",
    "\n",
    "\n",
    "#### ___Additonal Information___\n",
    "\n",
    "__To create a .yml file, execute the following commands:__\n",
    "\n",
    "- Activate the environment to export: \n",
    "    - __conda activate myenv__\n",
    "\n",
    "- Export your active environment to a new file:\n",
    "    - __conda env export > [ENVIRONMENT FILENAME].yml__\n",
    "\n",
    "\n",
    "### __Reference(s)__\n",
    "Latest UFS Weather Model Guide:\n",
    "- https://ufs-weather-model.readthedocs.io/en/latest/InputsOutputs.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data directories from RDHPCS & filter to only timestamps tracked by bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progress_bar import ProgressPercentage\n",
    "import os \n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class GetTimestampData():\n",
    "    \"\"\"\n",
    "    Extract locality of the UFS datasets of interest & generate a dictionary which will\n",
    "    map the UFS dataset files into the following dataset types:\n",
    "    Input data, WW3 input data, Baseline data, and BMIC data. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hpc_dir, avoid_fldrs, tracker_log_file=\"./data_from_ts_tracker/latest_rt.sh.pk\"):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            hpc_dir (str): Root directory path of where all the UFS timestamp datasets reside.\n",
    "            avoid_fldrs (str): Foldername to ignore within main directory of interest on-prem.\n",
    "                               Note: Some data folders were found w/ people's names within\n",
    "                               them -- to be ignored.\n",
    "            tracker_log_file (str): The folder directory containing the return of the UFS data \n",
    "                                    tracker bot.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Datasets' main directory of interest. \n",
    "        self.hpc_dir = hpc_dir\n",
    "        \n",
    "        # Extract all data directories residing w/in datasets' main hpc directory.\n",
    "        # Remove file directories comprise of a folder name.\n",
    "        self.avoid_fldrs = avoid_fldrs\n",
    "        self.file_dirs = self.get_data_dirs()\n",
    "        \n",
    "        # List of all data file directories w/in the UFS datasets.\n",
    "        self.partition_datasets = self.get_input_bl_data()\n",
    "        \n",
    "        # Read timestamps recorded by the UFS tracker bot.\n",
    "        self.tracker_log_file = tracker_log_file\n",
    "        with open(self.tracker_log_file, 'rb') as log_file:\n",
    "            self.data_log_dict = pickle.load(log_file)\n",
    "        \n",
    "        # Filter data directory paths to timestamps recorded by the UFS data tracker bot.\n",
    "        # For bot, refer to https://github.com/NOAA-EPIC/ufs-dev_data_timestamps.\n",
    "        self.filter2tracker_ts_datasets = self.get_tracker_ts_files()\n",
    "        \n",
    "        # Data files pertaining to specific timestamps of interest.\n",
    "        # Select timestamp dataset(s) to transfer from RDHPCS on-disk to cloud\n",
    "        #self.filter2specific_ts_datasets = self.get_specific_ts_files()\n",
    "        \n",
    "        # List of all data folders/files in datasets' main directory of interest.\n",
    "        self.rt_root_list = os.listdir(self.hpc_dir)\n",
    "        print(\"\\033[1m\" +\\\n",
    "              f\"All Primary Dataset Folders & Files In Main Directory ({self.hpc_dir}):\" +\\\n",
    "              f\"\\n\\n\\033[0m{self.rt_root_list}\")\n",
    "        \n",
    "    def get_data_dirs(self):\n",
    "        \"\"\"\n",
    "        Extract list of all file directories in datasets' main directory.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "            \n",
    "        Return (list): List of all file directories in datasets' main directory\n",
    "        of interest.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate list of all file directories residing w/in datasets' \n",
    "        # main directory of interest. \n",
    "        file_dirs = []\n",
    "        file_size =[]\n",
    "        for root_dir, subfolders, filenames in os.walk(self.hpc_dir):\n",
    "            for file in filenames:\n",
    "                file_dirs.append(os.path.join(root_dir, file))\n",
    "        \n",
    "        # Removal of personal names.\n",
    "        if self.avoid_fldrs != None:\n",
    "            file_dirs = [x for x in file_dirs if any(x for name in self.avoid_fldrs if name not in x)]\n",
    "        \n",
    "        return file_dirs\n",
    "\n",
    "    def get_input_bl_data(self):\n",
    "        \"\"\"\n",
    "        Extract list of all input file & baseline file directories.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "            \n",
    "        Return (dict): Dictionary partitioning the file directories into the\n",
    "        dataset types.\n",
    "        \n",
    "        *Note: Will keep 'INPUTDATA_ROOT_WW3' as a key wihtin the mapped dictionary\n",
    "        -- in case, the NOAA development team decides to migrate WW3_input_data_YYYYMMDD\n",
    "        out of the input-data-YYYYMMDD folder then, we will need to track the \n",
    "        'INPUTDATA_ROOT_WW3' related data files.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract list of all input file & baseline file directories.\n",
    "        partition_datasets = defaultdict(list) \n",
    "        for file_dir in self.file_dirs:\n",
    "\n",
    "            # Input data files w/ root directory truncated.\n",
    "            if any(subfolder in file_dir for subfolder in ['input-data', 'INPUT-DATA']):\n",
    "                partition_datasets['INPUTDATA_ROOT'].append(file_dir.replace(self.hpc_dir, \"\"))\n",
    "\n",
    "            # Baseline data files w/ root directory truncated.\n",
    "            if any(subfolder in file_dir for subfolder in ['develop', 'ufs-public-release', 'DEVELOP', 'UFS-PUBLIC-RELEASE']):\n",
    "                partition_datasets['BL_DATE'].append(file_dir.replace(self.hpc_dir, \"\"))\n",
    "                \n",
    "            # WW3 input data files w/ root directory truncated.\n",
    "            if any(subfolder in file_dir for subfolder in ['WW3_input_data', 'ww3_input_data', 'WW3_INPUT_DATA']):\n",
    "                partition_datasets['INPUTDATA_ROOT_WW3'].append(file_dir.replace(self.hpc_dir, \"\"))\n",
    "                \n",
    "            # BM IC input data files w/ root directory truncated.\n",
    "            if any(subfolder in file_dir for subfolder in ['BM_IC', 'bm_ic']):\n",
    "                partition_datasets['INPUTDATA_ROOT_BMIC'].append(file_dir.replace(self.hpc_dir, \"\"))\n",
    "\n",
    "\n",
    "        return partition_datasets    \n",
    "    \n",
    "    def get_tracker_ts_files(self):\n",
    "        \"\"\"\n",
    "        Filters file directory paths related to timestamps obtained from UFS data tracker bot.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "\n",
    "        Return (dict): Dictionary partitioning file directories into the\n",
    "        timestamps of interest obtained from UFS data tracker bot.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Reference timestamps captured from data tracker.\n",
    "        filter2tracker_ts_datasets = defaultdict(list) \n",
    "        for dataset_type, timestamps in self.data_log_dict.items():\n",
    "            \n",
    "            # Extracts datafiles within the timestamps captured from data tracker.\n",
    "            if dataset_type == 'INPUTDATA_ROOT':\n",
    "                for subfolder in self.partition_datasets[dataset_type]:\n",
    "                    if any(ts in subfolder for ts in timestamps):\n",
    "                        filter2tracker_ts_datasets[dataset_type].append(subfolder)\n",
    "\n",
    "            if dataset_type == 'BL_DATE':\n",
    "                for subfolder in self.partition_datasets[dataset_type]:\n",
    "                    if any(ts in subfolder for ts in timestamps):\n",
    "                        filter2tracker_ts_datasets[dataset_type].append(subfolder)\n",
    "\n",
    "            if dataset_type == 'INPUTDATA_ROOT_WW3':\n",
    "                for subfolder in self.partition_datasets[dataset_type]:\n",
    "                    if any(ts in subfolder for ts in timestamps):\n",
    "                        filter2tracker_ts_datasets[dataset_type].append(subfolder)\n",
    "\n",
    "            if dataset_type == 'INPUTDATA_ROOT_BMIC':\n",
    "                for subfolder in self.partition_datasets[dataset_type]:\n",
    "                    if any(ts in subfolder for ts in timestamps):\n",
    "                        filter2tracker_ts_datasets[dataset_type].append(subfolder)\n",
    "                        \n",
    "        return filter2tracker_ts_datasets\n",
    "    \n",
    "    def get_specific_ts_files(self, input_ts, bl_ts, ww3_input_ts, bmic_ts):\n",
    "        \"\"\"\n",
    "        Filters directory paths to timestamps of interest.\n",
    "        \n",
    "        Args: \n",
    "            input_ts (list): List of input timestamps to upload to cloud.\n",
    "            bl_ts (list): List of baseline timestamps to upload to cloud.\n",
    "            ww3_input_ts (list): List of WW3 input timestamps to upload to cloud.\n",
    "            bmic_ts (list): List of BMIC timestamps to upload to cloud.\n",
    "                                  \n",
    "        Return (dict): Dictionary partitioning the file directories into the\n",
    "        timestamps of interest specified by user.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Create dictionary mapping the user's request of timestamps.\n",
    "        specific_ts_dict = defaultdict(list)\n",
    "        specific_ts_dict['INPUTDATA_ROOT'] = input_ts\n",
    "        specific_ts_dict['BL_DATE'] = bl_ts\n",
    "        specific_ts_dict['INPUTDATA_ROOT_WW3'] = ww3_input_ts\n",
    "        specific_ts_dict['INPUTDATA_ROOT_BMIC'] = bmic_ts\n",
    "        \n",
    "        # Filter to directory paths of the timestamps specified by user.\n",
    "        filter2specific_ts_datasets = defaultdict(list) \n",
    "        for dataset_type, timestamps in specific_ts_dict.items():\n",
    "            \n",
    "            # Extracts data files within the timestamps captured from data tracker.\n",
    "            if dataset_type == 'INPUTDATA_ROOT':\n",
    "                for subfolder in self.partition_datasets[dataset_type]:\n",
    "                    if any(ts in subfolder for ts in timestamps):\n",
    "                        filter2specific_ts_datasets[dataset_type].append(subfolder)\n",
    "\n",
    "            if dataset_type == 'BL_DATE':\n",
    "                for subfolder in self.partition_datasets[dataset_type]:\n",
    "                    if any(ts in subfolder for ts in timestamps):\n",
    "                        filter2specific_ts_datasets[dataset_type].append(subfolder)\n",
    "\n",
    "            if dataset_type == 'INPUTDATA_ROOT_WW3':\n",
    "                for subfolder in self.partition_datasets[dataset_type]:\n",
    "                    if any(ts in subfolder for ts in timestamps):\n",
    "                        filter2specific_ts_datasets[dataset_type].append(subfolder)\n",
    "\n",
    "            if dataset_type == 'INPUTDATA_ROOT_BMIC':\n",
    "                for subfolder in self.partition_datasets[dataset_type]:\n",
    "                    if any(ts in subfolder for ts in timestamps):\n",
    "                        filter2specific_ts_datasets[dataset_type].append(subfolder)\n",
    "                        \n",
    "        return filter2specific_ts_datasets    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Datasets to Support Developing UFS Models of Interest\n",
    "\n",
    "#### Test Sample\n",
    "The script will read from the data timestamp tracker bot's output pickle file. Currently, the test sample generated by the  data timestamp tracker bot resides in **./data_from_ts_tracker/latest_rt.sh.pk**\n",
    "\n",
    "\n",
    "| UFS MODEL DEVELOPMENT VERSIONS| BASELINE DATA | INPUT DATA | WW3 INPUT DATA | BM_IC |\n",
    "| :- | :- | :- | -: | :-: |\n",
    "| Supports UFS Model Version Deployed in CSPs| 20220207 | input-data-20211210  | WW3_input_data_20211113 | BM_IC-20210717 |\n",
    "| Supports NOAA Dev Team Versons (since 03-18-22)| 20220318 | input-data-20211210  | WW3_input_data_20211113 | BM_IC-20220207 |\n",
    "| Supports NOAA Dev Team Versions (since 03-18-22)| 20220321 | input-data-20211210  | WW3_input_data_20211113 | BM_IC-20220207 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain directories for the datasets tracked by the data tracker bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAll Primary Dataset Folders & Files In Main Directory (/home/schin/work/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/):\n",
      "\n",
      "\u001b[0m['develop-20220321', 'develop-20220222', 'develop-20220329', 'develop-20220318', 'develop-20220207', 'ufs-public-release-v2-20210212', 'develop-20220406', 'develop-20220214', 'ufs-public-release-v2-20210208', 'develop-20220120', 'develop-20220322', 'develop-20220121', 'BM_IC-20220207', 'develop-20220316', 'develop-20220224', 'develop-20220401', 'develop-20220325', 'develop-20220210', 'develop-20220408', 'develop-20220228', 'develop-20220215', 'develop-20220217', 'input-data-20211210', 'develop-20220328', 'develop-20220304', 'BM_IC-20210717']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    \n",
    "    # Establish locality of where the dataseta are sourced.\n",
    "    linked_home_dir = \"/home/schin/work\"\n",
    "    orion_rt_data_dir = linked_home_dir + \"/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/\"\n",
    "\n",
    "    # Filter to tracker log's timestamps & extract their corresponding UFS input & baseline file directories.\n",
    "    filter2tracker_ts_datasets = GetTimestampData(orion_rt_data_dir, None).filter2tracker_ts_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BL_DATE', 'INPUTDATA_ROOT', 'INPUTDATA_ROOT_WW3', 'INPUTDATA_ROOT_BMIC'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter2tracker_ts_datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain directories for the datasets requested by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAll Primary Dataset Folders & Files In Main Directory (/home/schin/work/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/):\n",
      "\n",
      "\u001b[0m['develop-20220321', 'develop-20220222', 'develop-20220329', 'develop-20220318', 'develop-20220207', 'ufs-public-release-v2-20210212', 'develop-20220406', 'develop-20220214', 'ufs-public-release-v2-20210208', 'develop-20220120', 'develop-20220322', 'develop-20220121', 'BM_IC-20220207', 'develop-20220316', 'develop-20220224', 'develop-20220401', 'develop-20220325', 'develop-20220210', 'develop-20220408', 'develop-20220228', 'develop-20220215', 'develop-20220217', 'input-data-20211210', 'develop-20220328', 'develop-20220304', 'BM_IC-20210717']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    \n",
    "    # Establish locality of where the dataseta are sourced.\n",
    "    linked_home_dir = \"/home/schin/work\"\n",
    "    orion_rt_data_dir = linked_home_dir + \"/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/\"\n",
    "    \n",
    "    # Select timestamp dataset to transfer from RDHPCS on-disk to cloud\n",
    "    input_ts, bl_ts, ww3_input_ts, bmic_ts = [], ['develop-20220406'], [], []\n",
    "    filter2specific_ts_datasets = GetTimestampData(orion_rt_data_dir, None).get_specific_ts_files(input_ts, bl_ts, ww3_input_ts, bmic_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BL_DATE'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter2specific_ts_datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multipart Upload of a file via boto3\n",
    "__multipart_threshold:__ The transfer size threshold for which multi-part uploads, downloads, and copies will automatically be triggered.\n",
    "\n",
    "__max_concurrency:__ The maximum number of threads that will be making requests to perform a transfer. If use_threads is set to False, the value provided is ignored as the transfer will only ever use the main thread.\n",
    "\n",
    "__multipart_chunksize:__ The partition size of each part for a multi-part transfer.\n",
    "\n",
    "__use_threads:__ If True, threads will be used when performing S3 transfers. If False, no threads will be used in performing transfers: all logic will be ran in the main thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures multi-part upload & makes use of threading to speed up performance.\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import botocore\n",
    "\n",
    "# Create S3 resource to connect to S3 via SDK\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Analyze data transferring performance.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "class UploadData():\n",
    "    \"\"\"\n",
    "    Upload datasets of interest to cloud data storage.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, hpc_dir, file_relative_dirs, use_bucket):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            hpc_dir (str): Root directory path of where all the UFS timestamp datasets reside.\n",
    "            file_relative_dirs (list): List of relative directory paths on-prem to obtain \n",
    "                                       the dataset files.\n",
    "            use_bucket (str): If set to 'rt', datasets will be uploaded to the cloud data\n",
    "                              storage bucket designated for the UFS RT datasets.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Main on-prem directory to locate the datasets. \n",
    "        self.hpc_dir = hpc_dir\n",
    "        \n",
    "        # List of data files' relative directory paths on-prem. \n",
    "        self.file_relative_dirs = file_relative_dirs\n",
    "        \n",
    "        if use_bucket == 'rt':\n",
    "            self.bucket_name = 'noaa-ufs-regtests-pds'\n",
    "        else:\n",
    "            print(f\"{use_bucket} Bucket Does Not Exist.\")\n",
    "\n",
    "    def upload_single_file(self, file_dir):\n",
    "        \"\"\"\n",
    "        Upload a single data file to cloud w/ an established API configuraton.\n",
    "\n",
    "        Args:\n",
    "            file_dir (str): Relative directory path of the data file to transfer to \n",
    "                            cloud data storage.\n",
    "            \n",
    "        Return: None\n",
    "        \n",
    "        The AWS SDK uploader will manage data file retries and handle multipart as well as \n",
    "        non-multipart data transfers. To retain the current dataset\n",
    "        directory paths established on the RDPHPCS, Orion, each key of a data \n",
    "        file object will be set to their source directory path as designated on Orion. \n",
    "        Reason: Configured as sch to avoid altering too many variables within the \n",
    "        UFS-WM Regression Test scripts for when the UFS-WM Regression Test framework \n",
    "        begins to establish an experimental directory for the transferring of UFS data\n",
    "        files from and to the RDHPCS on-prem disk and user's experimental directory, respectively.\n",
    "        \n",
    "        **TODO** If utilizing Jupyter Notebook, set NotebookApp.iopub_data_rate_limit=1.0e10 w/in \n",
    "        the configuration file: \"/.jupyter/jupyter_notebook_config.py\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Configuration for multipart upload.\n",
    "        KB, MB, GB = 1024, 1024**2, 1024**3\n",
    "\n",
    "        start_time = time.time()\n",
    "        config = TransferConfig(multipart_threshold=100*MB,\n",
    "                                max_concurrency=10,\n",
    "                                multipart_chunksize=50000*KB,\n",
    "                                num_download_attempts=2,\n",
    "                                use_threads=True)\n",
    "\n",
    "        # Upload file w/out extra arguments.\n",
    "        # Track multi-part upload progress current percentage, total, remaining size, etc\n",
    "        key_path=file_dir\n",
    "        s3.meta.client.upload_file(self.hpc_dir + file_dir,\n",
    "                                   self.bucket_name,\n",
    "                                   key_path,\n",
    "                                   Config=config,\n",
    "                                   Callback=ProgressPercentage(self.hpc_dir + file_dir))\n",
    "        \n",
    "        # Upload file w/ extra arguments.\n",
    "        #s3.meta.client.upload_file(self.hpc_dir + file_dir,\n",
    "        #                           self.bucket_name,\n",
    "        #                           key_path, \n",
    "        #                           ExtraArgs={'ACL': 'public-read', 'ContentType': 'text/nc'},\n",
    "        #                           Config=config,\n",
    "        #                           Callback=ProgressPercentage(self.hpc_dir + file_dir))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Processing time to upload file.\n",
    "        delta = (end_time-start_time)/60\n",
    "        print(f'Processing Time (min): {delta}\\n')\n",
    "\n",
    "        return \n",
    "    \n",
    "    def upload_files2cloud(self):\n",
    "        \"\"\"\n",
    "        Iterates through the list of data files' relative directory paths on-prem. \n",
    "\n",
    "        Args:\n",
    "            None\n",
    "            \n",
    "        Return: None\n",
    "        \n",
    "        *Note: Will be keeping 'INPUTDATA_ROOT_WW3' as a key within the mapped dictionary\n",
    "        -- in case, the NOAA development team decides restructure & migrate \n",
    "        'WW3_input_data_YYYYMMDD' out of the 'input-data-YYYYMMDD' folder then, will\n",
    "        need to track the 'INPUTDATA_ROOT_WW3' related data files.\n",
    "        \n",
    "        \"\"\"\n",
    "        for dataset_type, ts_files in self.file_relative_dirs.items():\n",
    "            for file_dir in ts_files:\n",
    "                self.upload_single_file(file_dir)\n",
    "                \n",
    "        return \n",
    "    \n",
    "    def multi_part_upload_with_s3_withTuning(self, file_dir, chunk_sz_list):\n",
    "        \"\"\"\n",
    "        Tuning API parameters for uploading a single data file to cloud data storage.\n",
    "\n",
    "        Args:\n",
    "            file_dir (str): Directory path of the file to transfer to cloud.\n",
    "            chunk_sz_list (list): List of the range of partition sizes to perform\n",
    "                                  multi-upload data transferrring.\n",
    "            \n",
    "        Return (pd.DataFrame): The amount of time it takes to transfer a given data file \n",
    "        versus the set chunksize.\n",
    "        \n",
    "        Used to configure the following API parameters -- in an effort to improve the uploading\n",
    "        performance of the UFS datasets to cloud. Note: The AWS SDK uploader will manage data\n",
    "        file retries and handle multipart as well as non-multipart data transfers.\n",
    "\n",
    "        API Parameters:\n",
    "        - __multipart_threshold:__ Transfer size threshold for which multipart uploads, downloads, \n",
    "        and copies will be automatically triggered against a given data file. Ensure multipart \n",
    "        uploads/downloads only happen if the size of a transfer is larger than the set \n",
    "        'multipart_threshold.'\n",
    "\n",
    "        - __max_concurrency:__ Maximum number of threads that will be making requests to perform\n",
    "        a data transfer. If 'use_threads' is set to False, the 'max_concurrency' value is ignored\n",
    "        since, the data transfer would then be set to using the single main thread.\n",
    "\n",
    "        - __multipart_chunksize:__ Partition size of each part of the data file when a multipart\n",
    "        transfer is being performed.\n",
    "\n",
    "        - __num_download_attempts:__ Number of download attempts retried upon errors when\n",
    "        downloading an object from the cloud data storage bucket. Note: These retries account for \n",
    "        errors for which occur when streaming data down from the cloud data storage such as \n",
    "        socket errors and read timeouts that may occur after receiving an 'OK' response from the cloud data\n",
    "        storage bucket. Exceptions such as throttling errors and 5xx errors are already\n",
    "        retried by botocore (default=5). The 'num_download_attempts' does not take into account the\n",
    "        number of exceptions retried by botocore.\n",
    "\n",
    "        - __max_io_queue:__ Maximum amount of read parts that can be queued in-memory to be written for a\n",
    "        download. The size of each of these read parts is at most the size of the 'io_chunksize.'\n",
    "        \n",
    "        - __io_chunksize:__ Maximum size of each chunk in the I/O queue.\n",
    "\n",
    "        - __use_threads:__ If set to True, worker threads will be used when performing S3 transfers. \n",
    "        If set to False, no additional worker threads will be used and data transfers will be be ran\n",
    "        via the single main thread.\n",
    "\n",
    "        - __max_bandwidth:__ Maximum bandwidth (int; bytes per second) that will be consumed in uploading\n",
    "        and downloading the file content.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #[For Tuning Configuring API Parameters].\n",
    "        #chunk_sz_list = [50000] #list(range(40000, 61000, 500)) # In KB\n",
    "\n",
    "        # Configuration API multipart upload.\n",
    "        KB, MB, GB = 1024, 1024**2, 1024**3\n",
    "        proc_time_list = []\n",
    "        for chunk_sz in chunk_sz_list:\n",
    "            print(f'Chunk Size: {chunk_sz}\\n')\n",
    "            start_time = time.time()\n",
    "            config = TransferConfig(multipart_threshold=100*MB,\n",
    "                                    max_concurrency=10,\n",
    "                                    multipart_chunksize=chunk_sz*KB,\n",
    "                                    num_download_attempts=2,\n",
    "                                    use_threads=True)\n",
    "\n",
    "\n",
    "            # Upload a file w/out extra arguments\n",
    "            # Track multi-part upload progress current percentage, total, remaining size, etc\n",
    "            key_path=file_dir\n",
    "            s3.meta.client.upload_file(self.hpc_dir + file_dir,\n",
    "                                       self.bucket_name,\n",
    "                                       key_path,\n",
    "                                       Config=config,\n",
    "                                       Callback=ProgressPercentage(self.hpc_dir + file_dir))\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Processing time to upload file.\n",
    "            delta = (end_time-start_time)/60\n",
    "            print(f'Processing Time (min): {delta}\\n')\n",
    "            proc_time_list.append(delta)\n",
    "        \n",
    "        # Log processing time to upload file and the corespond. set data partition size.\n",
    "        time2chunksz_df = pd.DataFrame([chunk_sz_list, proc_time_list], index=['chunk_sz', 'xfer_time']).T\n",
    "\n",
    "        return time2chunksz_df\n",
    "    \n",
    "    def purge(self, key_path):\n",
    "        \"\"\"\n",
    "        Remove data file object w/ the given key from cloud data storage.\n",
    "        \n",
    "        Args:\n",
    "            key_path (str): Key of the data file object w/in the cloud data storage.\n",
    "            \n",
    "        Return: None\n",
    "\n",
    "        \"\"\"\n",
    "        s3.Object(self.bucket_name, key_path).delete()\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload datasets tracked by the data tracker bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     linked_home_dir = \"/home/schin/work\"\n",
    "#     orion_rt_data_dir = linked_home_dir + \"/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/\"\n",
    "#     uploader_wrapper = UploadData(orion_rt_data_dir, filter2tracker_ts_datasets, use_bucket='rt')\n",
    "#     uploader_wrapper.upload_files2cloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload datasets by timestamps as requested by the user.\n",
    "- In this scenario, used when transferring data files required for the UFS-WM currently deployed in the CSPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    linked_home_dir = \"/home/schin/work\"\n",
    "    orion_rt_data_dir = linked_home_dir + \"/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/\"\n",
    "    uploader_wrapper = UploadData(orion_rt_data_dir, filter2specific_ts_datasets, use_bucket='rt')\n",
    "    uploader_wrapper.upload_files2cloud()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_specific_ts_datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete a File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     linked_home_dir = \"/home/schin/work\"\n",
    "#     orion_rt_data_dir = linked_home_dir + \"/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/\"\n",
    "#     uploader_wrapper = UploadData(orion_rt_data_dir, file_relative_dirs=None, use_bucket='rt')\n",
    "#     file_dir = 'input-data-20211210/fv3_regional_c768/INPUT/grid.tile7.halo4.nc'\n",
    "#     key_path = file_dir\n",
    "#     uploader_wrapper.purge(key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload a Single Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     linked_home_dir = \"/home/schin/work\"\n",
    "#     orion_rt_data_dir = linked_home_dir + \"/noaa/nems/emc.nemspara/RT/NEMSfv3gfs/\"\n",
    "#     uploader_wrapper = UploadData(orion_rt_data_dir, file_relative_dirs=None, use_bucket='rt')\n",
    "#     file_dir = 'input-data-20211210/fv3_regional_c768/INPUT/grid.tile7.halo4.nc'\n",
    "#     uploader_wrapper.upload_single_file(file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm the NetCDF file was transferred properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a file from S3\n",
    "# If the service returns a 404 error, it prints an error message indicating that the object doesn't exist.\n",
    "\n",
    "KEY =  'input-data-20211210/fv3_regional_c768/INPUT/grid.tile7.halo4.nc'#key_path # replace with your object key\n",
    "BUCKET_NAME = 'noaa-ufs-regtests-pds'\n",
    "try:\n",
    "    s3.Bucket(BUCKET_NAME).download_file(KEY, 'test.nc')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        print(\"The object does not exist.\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://joehamman.com/2013/10/12/plotting-netCDF-data-with-Python/\n",
    "#from mpl_toolkits.basemap import Basemap \n",
    "\n",
    "my_example_nc_file = './test.nc'\n",
    "fh = Dataset(my_example_nc_file, mode='r')\n",
    "# lons = fh.variables['lon'][:]\n",
    "# lats = fh.variables['lat'][:]\n",
    "# lsoil = fh.variables['lsoil'][:]\n",
    "# geolon = fh.variables['geolon'][:]\n",
    "\n",
    "fh\n",
    "# lons\n",
    "#lats\n",
    "# lsoil\n",
    "# geolon \n",
    "#print(fh['geolon'].shape) \n",
    "print(fh['tile'].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh.variables['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_local_file = orion_rt_data_dir + 'input-data-20211210/fv3_regional_c768/INPUT/grid.tile7.halo4.nc'\n",
    "print(rt_local_file)\n",
    "fh2 = Dataset(rt_local_file, mode='r')\n",
    "# lons2 = fh2.variables['lon'][:]\n",
    "# lats2 = fh2.variables['lat'][:]\n",
    "# lsoil2 = fh2.variables['lsoil'][:]\n",
    "# geolon2 = fh2.variables['geolon'][:]\n",
    "\n",
    "fh2\n",
    "# lons2\n",
    "#lats2\n",
    "# lsoil2\n",
    "# geolon2\n",
    "#print(fh2['geolon'].shape) \n",
    "print(fh2['tile'].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh2.variables['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket.get_bucket('noaa-ufs-regtests-pds').lookup('input-data-20211210/DATM_ERA5_input_data/TL639_200618_ESMFmesh.nc')\n",
    "print(key.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning: Multiupload params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesize = '100 MB'\n",
    "\n",
    "plt.plot(time2chunksz_df['chunk_sz'], time2chunksz_df['xfer_time'])\n",
    "\n",
    "# Add title and axis names\n",
    "plt.title(f'Single {filesize} File:\\nUpload Time vs Chunk Size')\n",
    "plt.xlabel('Chunk Size (KB)')\n",
    "plt.ylabel('Upload Time (mins)')\n",
    " \n",
    "# Create names on the x axis\n",
    "plt.xticks()\n",
    " \n",
    "# Show graph\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'UploadTime_vs_ChunkSize_{filesize}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(40000, 61000, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    usr_root_dir = orion_rt_data_dir\n",
    "    file_dir = 'input-data-20211210/fv3_regional_c768/INPUT/gfs_data.nc'\n",
    "    BUCKET_NAME = 'noaa-ufs-regtests-pds'\n",
    "    key_path = file_dir\n",
    "    chunk_sz_list = list(range(40000, 61000, 10000)) # In KB #[50000]\n",
    "    time2chunksz_df = multi_part_upload_with_s3(usr_root_dir + file_dir, BUCKET_NAME, key_path, chunk_sz_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesize = '8 GB'\n",
    "\n",
    "plt.plot(time2chunksz_df['chunk_sz'], time2chunksz_df['xfer_time'])\n",
    "\n",
    "# Add title and axis names\n",
    "plt.title(f'Single {filesize} File:\\nUpload Time vs Chunk Size')\n",
    "plt.xlabel('Chunk Size (KB)')\n",
    "plt.ylabel('Upload Time (mins)')\n",
    " \n",
    "# Create names on the x axis\n",
    "plt.xticks()\n",
    " \n",
    "# Show graph\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'UploadTime_vs_ChunkSize_{filesize}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    usr_root_dir = orion_rt_data_dir\n",
    "    file_dir = 'input-data-20211210/fv3_regional_c768/INPUT/grid.tile7.halo4.nc'\n",
    "    BUCKET_NAME = 'noaa-ufs-regtests-pds'\n",
    "    key_path = file_dir\n",
    "    chunk_sz_list = list(range(40000, 10000, 1000)) # In KB #[50000]\n",
    "    time2chunksz_df = multi_part_upload_with_s3(usr_root_dir + file_dir, BUCKET_NAME, key_path, chunk_sz_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesize = '0.5 GB'\n",
    "\n",
    "plt.plot(time2chunksz_df['chunk_sz'], time2chunksz_df['xfer_time'])\n",
    "\n",
    "# Add title and axis names\n",
    "plt.title(f'Single {filesize} File:\\nUpload Time vs Chunk Size')\n",
    "plt.xlabel('Chunk Size (KB)')\n",
    "plt.ylabel('Upload Time (mins)')\n",
    " \n",
    "# Create names on the x axis\n",
    "plt.xticks()\n",
    " \n",
    "# Show graph\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'UploadTime_vs_ChunkSize_{filesize}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_root_dir = orion_rt_data_dir\n",
    "file_dir = 'input-data-20211210/fv3_regional_c768/INPUT/gfs_data.nc'\n",
    "BUCKET_NAME = 'noaa-ufs-regtests-pds'\n",
    "key_path = file_dir\n",
    "start_time = time.time()\n",
    "s3.Bucket(BUCKET_NAME).upload_file(usr_root_dir + file_dir,  key_path)\n",
    "end_time = time.time()\n",
    "print(\"\\nProcessing Time (min):\", (end_time-start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferwrwewe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud_xfer",
   "language": "python",
   "name": "cloud_xfer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
